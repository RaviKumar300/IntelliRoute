{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "login(token=\"hf-YOUR_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading fine-tuned classifier and other Specialized models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    default_data_collator,\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device_map = \"auto\"\n",
    "\n",
    "models_info = {\n",
    "    \"coding_codellama\": \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    \"summary_llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"chat_mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"math_llemma\": \"EleutherAI/llemma_7b\"\n",
    "}\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, model_id in models_info.items():\n",
    "    print(f\"ðŸ”„ Loading {name} from {model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "\n",
    "    tokenizers[name] = tokenizer\n",
    "    models[name] = model\n",
    "    print(f\"âœ… Loaded {name}\")\n",
    "\n",
    "classifier_model_name = 'meta-llama/Llama-3.2-1B'\n",
    "adapter_path = './llama3.2-lora-tuned-adapter-query'\n",
    "\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(classifier_model_name, trust_remote_code=True)\n",
    "\n",
    "classifier_base = AutoModelForCausalLM.from_pretrained(\n",
    "    classifier_model_name,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    classifier_model_name,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "classifier_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "classifier_model = classifier_model.merge_and_unload().eval()\n",
    "\n",
    "if classifier_tokenizer.pad_token is None:\n",
    "    classifier_tokenizer.pad_token = classifier_tokenizer.eos_token\n",
    "\n",
    "print(\"âœ… Loaded classifier model: classifier_lora\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['instruction'], batch['response'])\n",
    "    ]\n",
    "    tokens = classifier_tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "    return tokens\n",
    "\n",
    "eval_ds = load_dataset('json', data_files='/kaggle/working/sample_2.jsonl')['train']\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['instruction', 'response'])\n",
    "eval_ds = eval_ds.with_format('torch')\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=8,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop words will be removed before storing in context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main conversation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "context_file = 'cxt.json'\n",
    "max_tokens = 4096  # context window for most models\n",
    "\n",
    "def clean_response(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "def load_context():\n",
    "    if not os.path.exists(context_file):\n",
    "        return []\n",
    "    with open(context_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_context(context):\n",
    "    with open(context_file, 'w') as f:\n",
    "        json.dump(context, f, indent=2)\n",
    "\n",
    "def truncate_context(tokenizer, context, query):\n",
    "    all_text = ''.join([f\"<s>[INST] {item['user']} [/INST] {item['assistant']} </s>\" for item in context])\n",
    "    all_text += f\"<s>[INST] {query} [/INST]\"\n",
    "    tokens = tokenizer(all_text, return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "\n",
    "    while len(tokens) > max_tokens and context:\n",
    "        context.pop(0)\n",
    "        all_text = ''.join([f\"<s>[INST] {item['user']} [/INST] {item['assistant']} </s>\" for item in context])\n",
    "        all_text += f\"<s>[INST] {query} [/INST]\"\n",
    "        tokens = tokenizer(all_text, return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "\n",
    "    return context\n",
    "\n",
    "def start_chat_session(models, tokenizers, classifier_model, classifier_tokenizer):\n",
    "    print(\"Chat started! Type 'quit' to exit.\\n\")\n",
    "\n",
    "    context = load_context()\n",
    "\n",
    "    classifier_pipeline = pipeline(\"text-generation\", model=classifier_model, tokenizer=classifier_tokenizer)\n",
    "\n",
    "    label_map = {\n",
    "        'coding': 'coding_codellama',\n",
    "        'summary': 'summary_llama3',\n",
    "        'chat': 'chat_mistral',\n",
    "        'math': 'math_llemma'\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "\n",
    "        # 1. Classify the query\n",
    "        class_prompt = f\"### Instruction:\\nclassify the following query into one of these: coding, summary, chat, math\\n### query: {user_input}\\n### Response:\\n\"\n",
    "        class_output = classifier_pipeline(class_prompt, max_new_tokens=10)[0]['generated_text']\n",
    "        predicted_label = class_output.split(\"### Response:\")[-1].strip().split()[0].lower()\n",
    "\n",
    "        if predicted_label not in label_map:\n",
    "            print(\"Couldn't classify query. Defaulting to chat.\")\n",
    "            predicted_label = 'chat'\n",
    "\n",
    "        model_key = label_map[predicted_label]\n",
    "        model = models[model_key]\n",
    "        tokenizer = tokenizers[model_key]\n",
    "\n",
    "        # 2. Build context prompt\n",
    "        context = truncate_context(tokenizer, context, user_input)\n",
    "        prompt = ''.join([f\"<s>[INST] {item['user']} [/INST] {item['assistant']} </s>\" for item in context])\n",
    "        prompt += f\"<s>[INST] {user_input} [/INST]\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "        print(f\"\\n{model_key} â†’ {predicted_label.upper()} Response:\\n{response}\\n\")\n",
    "\n",
    "        # 5. Remove stopwords and store\n",
    "        clean_resp = clean_response(response)\n",
    "\n",
    "        context.append({\n",
    "            \"user\": user_input,\n",
    "            \"assistant\": clean_resp\n",
    "        })\n",
    "\n",
    "        save_context(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_chat_session(models, tokenizers, classifier_model, classifier_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
