{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "adapter_path = './llama3.2-lora-tuned-adapter-query'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# Load model for LoRA merging\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Merge LoRA adapter\n",
    "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "tuned_model = tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['instruction'], batch['response'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset('json', data_files='/kaggle/working/sample_2.jsonl')['train']\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['instruction', 'response'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model):\n",
    "    losses = []\n",
    "    \n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return math.exp(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "raw_data = load_dataset('json', data_files='sample_2.jsonl')['train']\n",
    "refs = raw_data['response']\n",
    "\n",
    "\n",
    "# def generate(model, instruction):\n",
    "#     token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "#     return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "#     # return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def generate(model, instruction):\n",
    "    input = tokenizer(\n",
    "        f'### Instruction:\\n{instruction}\\n### Response:\\n',\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    input_ids = input['input_ids'].to('cuda')\n",
    "    attention_mask = input['attention_mask'].to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tuned = []\n",
    "res_base = []\n",
    "for i in range(df.shape[0]):\n",
    "    r1 = (generate(tuned_model, df[\"instruction\"][i]))\n",
    "    r2 = (generate(base_model, df[\"instruction\"][i]))\n",
    "    res_tuned.append(r1)\n",
    "    res_base.append(r2)\n",
    "    print(i, \"tuned: \",r1, \"\\n\", \"base: \", r2[:10])\n",
    "df[\"res_tuned\"] = res_tuned\n",
    "df[\"res_base\"] = res_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"math\", \"conversation\", \"coding\", \"summary\"]\n",
    "for i in range(len(res_tuned)):\n",
    "    if res_tuned[i] in classes:\n",
    "        continue\n",
    "    res_tuned[i] = \"\"\n",
    "    \n",
    "for i in range(len(res_base)):\n",
    "    if res_base[i] in classes:\n",
    "        continue\n",
    "    res_base[i] = \"\"\n",
    "\n",
    "df[\"base_pred\"] = res_base\n",
    "df[\"tuned_pred\"] = res_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tuned_pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ground truth\n",
    "y_test = df[\"response\"]\n",
    "\n",
    "# Base model predictions\n",
    "y_pred_base = df[\"base_pred\"]\n",
    "print(\"ðŸ“Š Base Model Evaluation:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm_base))\n",
    "\n",
    "# Plot confusion matrix for base model\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(y_test.unique()), yticklabels=sorted(y_test.unique()))\n",
    "plt.title(\"Confusion Matrix - Base Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Tuned model predictions\n",
    "y_pred_tuned = df[\"tuned_pred\"]\n",
    "print(\"\\n\\nðŸ“Š Tuned Model Evaluation:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm_tuned))\n",
    "\n",
    "# Plot confusion matrix for tuned model\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', xticklabels=sorted(y_test.unique()), yticklabels=sorted(y_test.unique()))\n",
    "plt.title(\"Confusion Matrix - Tuned Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
