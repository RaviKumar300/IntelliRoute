{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original environment: Kaggle nb, with GPU P100 active.\n",
    "Dataset (Compiled): https://www.kaggle.com/datasets/ravirvik/llm-question-classification-dataset/data\n",
    "\"\"\"\n",
    "\n",
    "# DataSources:\n",
    "#     programming  -- LLMA- Copilot/code\n",
    "#     https://www.kaggle.com/datasets/bhaveshmittal/python-programming-questions-dataset\n",
    "\n",
    "#     conversation/ nlu/ basic qna   --  Text Arena\n",
    "#     https://www.kaggle.com/datasets/lmsysorg/chatbot-arena-conversations\n",
    "\n",
    "#     summarization   -- Text Arena\n",
    "#     https://www.kaggle.com/datasets/pariza/bbc-news-summary\n",
    "\n",
    "#     maths/ reasoning   --  /math\n",
    "#     https://www.kaggle.com/datasets/awsaf49/math-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "main_df = pd.DataFrame(columns = [\"query\", \"class\"])\n",
    "\n",
    "coding = pd.read_csv(\"/kaggle/input/llm-question-classification-dataset/Python Programming Questions Dataset.csv\")\n",
    "# coding\n",
    "temp_df = pd.DataFrame(coding[\"Instruction\"])\n",
    "temp_df[\"class\"] = \"coding\"\n",
    "temp_df.columns = [\"query\", \"class\"]\n",
    "# temp_df\n",
    "main_df = pd.concat([main_df, temp_df], ignore_index=True)\n",
    "# main_df\n",
    "\n",
    "file_path = \"/kaggle/input/llm-question-classification-dataset/chatbot_arena_conversations.json\"\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass\n",
    "            # print(f\"Skipping line due to error: {e}\")\n",
    "# print(f\"Loaded {len(data)} records.\")\n",
    "# print(data[0][\"conversation_a\"][0][\"content\"])  # Preview first record\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    temp.append(data[i][\"conversation_a\"][0][\"content\"])\n",
    "temp_df = pd.DataFrame(columns = [\"query\", \"class\"])\n",
    "temp_df[\"query\"] = temp\n",
    "temp_df[\"class\"] = \"conversation\"\n",
    "main_df = pd.concat([main_df, temp_df], ignore_index=True)\n",
    "# main_df\n",
    "\n",
    "math = pd.read_csv(\"/kaggle/input/llm-question-classification-dataset/maths.csv\")\n",
    "# math\n",
    "main_df = pd.concat([main_df, math], ignore_index=True)\n",
    "# main_df\n",
    "\n",
    "summary = pd.read_csv(\"/kaggle/input/llm-question-classification-dataset/summary.csv\")\n",
    "main_df = pd.concat([main_df, summary], ignore_index=True)\n",
    "# main_df\n",
    "\n",
    "main_df[\"class\"].value_counts()\n",
    "# 33k, 13k, 7.5k, 2.2k ==> 55k!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE:\n",
    "#### To Balance different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "summary_df = main_df[main_df['class'] == 'summary']\n",
    "summary_df_duplicated = pd.concat([summary_df]*2, ignore_index=True)\n",
    "\n",
    "conversation_sample = main_df[main_df['class'] == 'conversation'].sample(n=4448, random_state=42)\n",
    "coding_sample = main_df[main_df['class'] == 'coding'].sample(n=4448, random_state=42)\n",
    "math_sample = main_df[main_df['class'] == 'math'].sample(n=4448, random_state=42)\n",
    "\n",
    "mini_df = pd.concat([summary_df_duplicated, conversation_sample, coding_sample, math_sample], ignore_index=True)\n",
    "\n",
    "mini_df = mini_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = mini_df\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to .jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in main_df.iterrows():\n",
    "        entry = {\n",
    "            \"instruction\": f\"classify the following query into one of following types: conversation, code, math, summary\\n ### query: {row['query']}\",\n",
    "            \"response\": row['class']\n",
    "        }\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "with open('/kaggle/working/prompt_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating 2 samples with 100 rows each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/prompt_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    lines = [json.loads(line) for line in f]\n",
    "\n",
    "first_100 = lines[:100]\n",
    "second_100 = lines[100:200]\n",
    "\n",
    "with open('sample_1.jsonl', 'w', encoding='utf-8') as f1:\n",
    "    for item in first_100:\n",
    "        f1.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('sample_2.jsonl', 'w', encoding='utf-8') as f2:\n",
    "    for item in second_100:\n",
    "        f2.write(json.dumps(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
